"""
1. select initial song in .krn and preprocessed (same song)
2. make seed with preprocessed style
3. see ML output in a preprocessed style
    3.1 now I have 2 strings: original and generated by ML, both in preprocessed style
4. Make comparison of both strings. EVALUATION.

5. Make it for same files used in train ML
6. Make with different files from other dataset

7. My own seed can be evaluated?

"""

# erk

#1699
"""
69 _ _ 71 72 _ _ _ 69 _ _ _ 64 _ _ _ 69 _ _ 71 69 _ _ _ 69 _ _ 71 72 _ _ _ 69 _ _ _ 64 _ _ _ 69 _ _ 71 69 _ _ _ 69 _ _ 72 71 _ _ _ 67 _ _ _ 64 _ _ _ 67 _ _ 69 67 _ _ _ 67 _ _ 69 71 _ _ _ 67 _ _ _ 66 _ _ _ 64 _ _ 66 64 _ r _ 60 _ _ 62 64 _ _ _ 64 _ _ _ 65 _ _ _ 67 _ _ 69 67 _ _ _ 69 _ _ 69 64 _ _ _ 64 _ _ _ 69 _ _ _ 71 _ _ 72 71 _ _ _ 69 _ _ 71 72 _ _ _ 69 _ _ _ 64 _ _ _ 69 _ _ 71 69 _ _ _
!!!OTL: Vrienden, kommt alle gaere
!!!ARE: Europa, Mitteleuropa, Deutschland
!!!SCT: L0091
!!!YEM: Copyright 1995, estate of Helmut Schaffrath.
**kern
*ICvox
*Ivox
*M3/4
*k[]
*a:
{8.a
16b
=1
4ccn
4a
4e
=2
8.a
16b
4a}
{8.a
16b
=3
4ccn
4a
4e
=4
8.a
16b
4a}
{8.a
16ccn
=5
4b
4gn
4e
=6
8.gn
16a
4gn}
{8.gn
16a
=7
4b
4gn
4f#
=8
8.e
16f#
8e
8r}
{8.cn
16d
=9
4e
4e
4fn
=10
8.gn
16a
4gn}
{8.a
16a
=11
4e
4e
4a
=12
8.b
16ccn
4b}
{8.a
16b
=13
4ccn
4a
4e
=14
8.a
16b
4a}
==
!!!AGN: Lied
!! Flaemisch, von der Familie Westendorp aus Elberfeld
!!!ONB: ESAC (Essen Associative Code) Database: ERK5
!!!AMT: simple triple
!!!AIN: vox
!!!EED: Helmut Schaffrath
!!!EEV: 1.0
*-
"""

#0
"""
55 _ 60 _ 60 _ 60 _ 62 _ 64 _ 62 _ 60 _ _ _ 64 _ 64 _ 64 _ 65 _ 67 _ _ 65 64 _ 60 _ 72 _ _ _ 72 71 69 _ 67 _ _ _ r _ 67 _ 69 _ 65 _ 62 _ 65 _ 67 _ 64 _ 60 _ 62 _ 64 _ 67 _ 65 _ 62 _ 60 _ _ _ r _ 67 _ 69 _ 65 _ 62 _ 65 _ 67 _ 64 _ 60 _ 62 _ 64 _ 67 _ 65 _ 62 _ 60 _ _ _ r _
!!!OTL: ES FREIT EIN WILDER WASSERMANN
!!!ARE: Europa, Mitteleuropa, Deutschland, Schlesien, Hainau; Liegnitz
!!!SCT: E0001
!!!YEM: Copyright 1995, estate of Helmut Schaffrath.
**kern
*ICvox
*Ivox
*M2/4
*k[b-]
*F:
{8c
=1
8f
8f
8f
8g
=2
8a
8g
4f}
=3
{8a
8a
8a
8b-
=4
8.cc
16b-
8a}
{8f
=5
4ff
16ff
16ee
8dd
=6
4cc
8r}
{8cc
=7
8dd
8b-
8g
8b-
=8
8cc
8a
8f}
{8g
=9
8a
8cc
8b-
8g
=10
4f
8r}
{8cc
=11
8dd
8b-
8g
8b-
=12
8cc
8a
8f}
{8g
=13
8a
8cc
8b-
8g
=14
4f
8r}
==
!!!AGN: Sage, Maerchen -, Zauber - Lied, Ballade ?
!!!ONB: ESAC (Essen Associative Code) Database: ERK1
!!!AMT: simple duple
!!!AIN: vox
!!!EED: Helmut Schaffrath
!!!EEV: 1.0
*-
"""
"""
#https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/
# evaluate the model
scores = model.evaluate(X[test], Y[test], verbose=0)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
cvscores.append(scores[1] * 100)
"""

from train import build_model
from matplotlib import pyplot as plt
print(build_model)
from preprocess import generate_training_sequences
import tensorflow.keras as keras




#Building and training LSTM Network with dataset from preprocessing,
#to generate melodies.
#Time series music. Generating/Generative music

import tensorflow.keras as keras
from preprocess import generate_training_sequences, SEQUENCE_LENGTH
#38 because length of vocabulary in this case (erk data file)
OUTPUT_UNITS = 18
#ERROR function used for training
LOSS = "sparse_categorical_crossentropy"
LEARNING_RATE = 0.001
#number the neurons in the internal layesr of the network
#list because possible more than one internal layer
#in this case only one hidden layer with 256 hidden neurons
NUM_UNITS =[256]
#2 layers
#NUM_UNITS =[256, 256]
#40 - 100 is okay number of epochs
EPOCHS = 3
#number of samples that Network will see before running backpropagation
BATCH_SIZE = 64
#.h5 because keras save models like that (it stores all the information of the model)
SAVE_MODEL_PATH = "model.h5"


#generating and compiling the model
def build_model(output_units,num_units, loss, learning_rate):
    #1.create the model arquitecture
    #output_units = vocabulary size
    input = keras.layers.Input(shape = (None, output_units))
    x = keras.layers.LSTM(num_units[0])(input)
    #Dropuout is a technique to avoid overfitting
    x = keras.layers.Dropout(0.2)(x)

    output = keras.layers.Dense(output_units, activation = "softmax")(x)
   
    #build the model after have input and output
    model = keras.Model(input, output)

    #2.compile model
    # https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/
    # https://keras.io/api/metrics/ 
    import tensorflow as tf
    model.compile(loss = loss, optimizer = keras.optimizers.Adam(lr = learning_rate), metrics = ["accuracy", "mse", "mape", 'mae', "sparse_categorical_accuracy", "msle", "categorical_accuracy",
    "binary_accuracy", "hinge", "sparse_top_k_categorical_accuracy", "cosine_proximity", "mean_squared_error", "mean_absolute_error", "mean_absolute_percentage_error", "top_k_categorical_accuracy",
    "sparse_categorical_crossentropy", "kullback_leibler_divergence", tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanSquaredLogarithmicError(), tf.keras.metrics.CosineSimilarity(axis=1),
    tf.keras.metrics.LogCoshError(), tf.keras.metrics.SquaredHinge(), tf.keras.metrics.CategoricalHinge() ])
    
    #print all the layers of the model(visual feedback)
    model.summary()

    return model


#to train the Network(high level function). Arguments have default values.
def train(output_units = OUTPUT_UNITS, num_units = NUM_UNITS, loss = LOSS,learning_rate= LEARNING_RATE):

    #generate the training sequences
    inputs, targets = generate_training_sequences(SEQUENCE_LENGTH)
    #build the network/graph (build the model)
    model = build_model(output_units, num_units, loss, learning_rate)
    #train the model
    model.fit(inputs, targets, epochs = EPOCHS, batch_size = BATCH_SIZE)
    #save the model(reuse, call back)
    model.save(SAVE_MODEL_PATH)

    history =  model.fit(inputs, targets, epochs = EPOCHS, batch_size = BATCH_SIZE)
    print(history.history.keys())
    # summarize history for accuracy
    plt.plot(history.history['accuracy'])
#    plt.plot(history.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()
    # summarize history for loss
    plt.plot(history.history['loss'])
#    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()



    from matplotlib import pyplot
    # plot metrics
    pyplot.plot(history.history['mse'])
    pyplot.savefig('mse.png')
    
    pyplot.plot(history.history['mape'])
    pyplot.savefig('mape.png')
    pyplot.plot(history.history['mae'])
    pyplot.plot(history.history['accuracy'])
    pyplot.plot(history.history['categorical_accuracy'])
    pyplot.plot(history.history['binary_accuracy'])


    
    pyplot.show()









if __name__== "__main__":
    train()
    
